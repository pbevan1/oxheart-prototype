{
  "components": {
    "comp-cross-validation": {
      "executorLabel": "exec-cross-validation",
      "inputDefinitions": {
        "artifacts": {
          "training_data": {
            "artifactType": {
              "schemaTitle": "system.Artifact",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "metrics_names": {
            "parameterType": "LIST"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "eval_metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "kpi": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "comp-extract-table-to-gcs": {
      "executorLabel": "exec-extract-table-to-gcs",
      "inputDefinitions": {
        "parameters": {
          "dataset_id": {
            "parameterType": "STRING"
          },
          "location": {
            "defaultValue": "EU",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "project_id": {
            "parameterType": "STRING"
          },
          "table_id": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "dataset": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "comp-feature-eng": {
      "executorLabel": "exec-feature-eng",
      "inputDefinitions": {
        "artifacts": {
          "data_input": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "col_label": {
            "parameterType": "STRING"
          },
          "col_training": {
            "parameterType": "LIST"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "dataset_train": {
            "artifactType": {
              "schemaTitle": "system.Artifact",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "shape_train": {
            "parameterType": "NUMBER_INTEGER"
          }
        }
      }
    },
    "comp-query-to-table": {
      "executorLabel": "exec-query-to-table",
      "inputDefinitions": {
        "parameters": {
          "dataset_id": {
            "parameterType": "STRING"
          },
          "location": {
            "defaultValue": "EU",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "project_id": {
            "parameterType": "STRING"
          },
          "query": {
            "parameterType": "STRING"
          },
          "query_job_config": {
            "isOptional": true,
            "parameterType": "STRUCT"
          },
          "table_id": {
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-register-model": {
      "executorLabel": "exec-register-model",
      "inputDefinitions": {
        "artifacts": {
          "model": {
            "artifactType": {
              "schemaTitle": "system.Model",
              "schemaVersion": "0.0.1"
            }
          },
          "model_metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "display_name": {
            "parameterType": "STRING"
          },
          "location": {
            "parameterType": "STRING"
          },
          "model_description": {
            "parameterType": "STRING"
          },
          "project_id": {
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-train-model": {
      "executorLabel": "exec-train-model",
      "inputDefinitions": {
        "artifacts": {
          "training_data": {
            "artifactType": {
              "schemaTitle": "system.Artifact",
              "schemaVersion": "0.0.1"
            }
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "model": {
            "artifactType": {
              "schemaTitle": "system.Model",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    }
  },
  "defaultPipelineRoot": "gs://oxheart/heart/pipelines/",
  "deploymentSpec": {
    "executors": {
      "exec-cross-validation": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "cross_validation"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'scikit-learn==1.4.2' 'pandas==2.2.2' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef cross_validation(\n    training_data: InputPath(),\n    metrics_names: list,\n    kpi: Output[Metrics],\n    eval_metrics: Output[Metrics],\n) -> None:\n    \"\"\"\n    Train a classification model and save it to Google Cloud Storage.\n    \"\"\"\n    import logging\n    import os\n    import pickle\n    import joblib\n    import numpy as np\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.model_selection import cross_val_score\n\n    logging.basicConfig(level=logging.INFO)\n\n    # Load the training data\n    with open(training_data + \".pkl\", \"rb\") as file:\n        train_data = pickle.load(file)\n\n    X_train = np.array(train_data[\"x_train\"], copy=True)\n    y_train = np.array(train_data[\"y_train\"], copy=True)\n\n    logging.info(f\"X_train shape: {X_train.shape}\")\n    logging.info(f\"y_train shape: {y_train.shape}\")\n\n    logging.info(\"Starting training...\")\n    clf = LogisticRegression(n_jobs=-1, random_state=42)\n    # accs = cross_val_score(clf, X_train, y_train, cv=5, scoring=\"accuracy\")\n    # f1s = cross_val_score(clf, X_train, y_train, cv=5, scoring=\"f1\")\n    # accuracy = accs.mean()\n    # f1 = f1s.mean()\n\n    metrics_dict = {}\n    for each_metric in metrics_names:\n        scores = cross_val_score(clf, X_train, y_train, cv=5, scoring=each_metric)\n        metric_val =scores.mean()\n        metrics_dict[f\"{each_metric}\"] = metric_val\n        kpi.log_metric(f\"{each_metric}\", metric_val)\n\n        # dumping kpi metadata to generate the metrics kpi\n        with open(kpi.path, \"w\") as f:\n            json.dump(kpi.metadata, f)\n        logging.info(f\"{each_metric}: {metric_val:.3f}\")\n\n    # dumping metrics_dict to generate the metrics table\n    with open(eval_metrics.path, \"w\") as f:\n        json.dump(metrics_dict, f)\n\n"
          ],
          "image": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu.py310"
        }
      },
      "exec-extract-table-to-gcs": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "extract_table_to_gcs"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-bigquery==3.20.1' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef extract_table_to_gcs(\n    project_id: str,\n    dataset_id: str,\n    table_id: str,\n    dataset: Output[Dataset],\n    location: str = \"EU\",\n) -> None:\n    \"\"\"\n    Extract a Big Query table into Google Cloud Storage.\n    \"\"\"\n\n    import logging\n    import os\n    import google.cloud.bigquery as bq\n\n    # Get table generated by previous component\n    full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n    table = bq.table.Table(table_ref=full_table_id)\n\n    # Initiate BigQuery client to connect with project\n    job_config = bq.job.ExtractJobConfig(**{})\n    client = bq.client.Client(project=project_id, location=location)\n\n    # Submit extract table job to store on GCS\n    extract_job = client.extract_table(table, dataset.uri)\n\n"
          ],
          "image": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu.py310"
        }
      },
      "exec-feature-eng": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "feature_eng"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'pandas==2.2.2' 'scikit-learn==1.4.2' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef feature_eng(\n    data_input: Input[Dataset],\n    dataset_train: OutputPath(),\n    col_label: str,\n    col_training: list,\n) -> NamedTuple(\"Outputs\", [(\"shape_train\", int)]):\n    \"\"\"\n    Split data into train and test sets.\n    \"\"\"\n\n    import logging\n    import pickle\n\n    import pandas as pd\n    from sklearn import model_selection\n\n    df = pd.read_csv(data_input.path)\n\n    # df.dropna(inplace=True)\n    df[\"max_heart_rate\"].fillna(df[\"max_heart_rate\"].median(), inplace=True)\n\n    logging.info(f\"[START] CREATE SETS, starts with an initial shape of {df.shape}\")\n\n    if len(df) != 0:\n        yy = df[col_label]\n\n        xx = df[col_training]\n\n        x_train_results = {\"x_train\": xx, \"y_train\": yy}\n\n        with open(dataset_train + f\".pkl\", \"wb\") as file:\n            pickle.dump(x_train_results, file)\n\n        logging.info(f\"[END] FEATURE ENG, training data set was created\")\n\n        return (len(xx))\n\n    else:\n        logging.error(f\"[END] FEATURE ENG, data set is empty\")\n        return (None)\n\n"
          ],
          "image": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu.py310"
        }
      },
      "exec-query-to-table": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "query_to_table"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-bigquery==3.20.1' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef query_to_table(\n    query: str,\n    project_id: str,\n    dataset_id: str,\n    table_id: str,\n    location: str = \"EU\",\n    query_job_config: dict = None,\n) -> None:\n    \"\"\"\n    Run the query and create a new BigQuery table\n    \"\"\"\n\n    import google.cloud.bigquery as bq\n\n    # Configure query job\n    job_config = bq.QueryJobConfig(\n        destination=f\"{project_id}.{dataset_id}.{table_id}\", **query_job_config\n    )\n\n    # Initiate BigQuery client\n    bq_client = bq.Client(project=project_id, location=location)\n\n    # Generate query with all job configs\n    query_job = bq_client.query(query, job_config=job_config)\n    query_job.result()\n\n    print(f\"Query job with ID {query_job.job_id} finished.\")\n\n"
          ],
          "image": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu.py310"
        }
      },
      "exec-register-model": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "register_model"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'pandas==2.2.2' 'google-cloud-aiplatform==1.48.0' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef register_model(\n    model: Input[Model],\n    model_metrics: Input[Metrics],\n    project_id: str,\n    location: str,\n    display_name: str,\n    model_description: str,\n) -> None:\n    from google.cloud import aiplatform\n    from google.cloud.aiplatform import gapic\n    import json\n\n    # Load metrics\n    with open(model_metrics.path) as f:\n        metrics = json.load(f)\n\n    # f1_dict = {k: v for k, v in metrics.items() if k == 'f1_score'}\n\n    # Initialize Vertex AI client\n    aiplatform.init(project=project_id, location=location)\n\n    # Upload the model to Vertex AI with metrics as labels\n    registered_model = aiplatform.Model.upload(\n        display_name=display_name,\n        artifact_uri=model.path,\n        description=model_description,\n        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-23:latest\",\n    )\n\n    # model_eval = gapic.ModelEvaluation(\n    #     display_name=\"eval\",\n    #     metrics_schema_uri=\"gs://google-cloud-aiplatform/schema/modelevaluation/classification_metrics_1.0.0.yaml\",\n    #     metrics=f1_dict,\n    # )\n\n    # API_ENDPOINT = f\"{location}-aiplatform.googleapis.com\"\n    # client = gapic.ModelServiceClient(client_options={\"api_endpoint\": API_ENDPOINT})\n\n    # client.import_model_evaluation(parent=registered_model.resource_name, model_evaluation=model_eval)\n\n    print(f\"Model registered with metrics. ID: {registered_model.resource_name}\")\n\n"
          ],
          "image": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu.py310"
        }
      },
      "exec-train-model": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "train_model"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'scikit-learn==1.4.2' 'pandas==2.2.2' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef train_model(\n    training_data: InputPath(),\n    model: Output[Model],\n) -> None:\n    \"\"\"\n    Train a classification model and save it to Google Cloud Storage.\n    \"\"\"\n    import logging\n    import os\n    import pickle\n    import joblib\n    import numpy as np\n    from sklearn.linear_model import LogisticRegression\n    # from google.cloud import storage\n\n    logging.basicConfig(level=logging.INFO)\n\n    # Load the training data\n    with open(training_data + \".pkl\", \"rb\") as file:\n        train_data = pickle.load(file)\n\n    X_train = np.array(train_data[\"x_train\"], copy=True)\n    y_train = np.array(train_data[\"y_train\"], copy=True)\n\n    logging.info(f\"X_train shape: {X_train.shape}\")\n    logging.info(f\"y_train shape: {y_train.shape}\")\n\n    logging.info(\"Starting training...\")\n    clf = LogisticRegression(n_jobs=-1, random_state=42)\n    train_model = clf.fit(X_train, y_train)\n\n    # Save the model to a temporary local file\n    temp_model_file = \"/tmp/model.joblib\"\n    joblib.dump(train_model, temp_model_file)\n\n    os.makedirs(model.path, exist_ok=True)\n    model_file = os.path.join(model.path, \"model.joblib\")\n    # Save the final model\n    logging.info(f\"Saving model to: {model_file}\")\n    joblib.dump(train_model, model_file)\n\n"
          ],
          "image": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu.py310"
        }
      }
    }
  },
  "pipelineInfo": {
    "name": "mf-oxheart-prototype"
  },
  "root": {
    "dag": {
      "outputs": {
        "artifacts": {
          "cross-validation-eval_metrics": {
            "artifactSelectors": [
              {
                "outputArtifactKey": "eval_metrics",
                "producerSubtask": "cross-validation"
              }
            ]
          },
          "cross-validation-kpi": {
            "artifactSelectors": [
              {
                "outputArtifactKey": "kpi",
                "producerSubtask": "cross-validation"
              }
            ]
          }
        }
      },
      "tasks": {
        "cross-validation": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-cross-validation"
          },
          "dependentTasks": [
            "feature-eng"
          ],
          "inputs": {
            "artifacts": {
              "training_data": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "dataset_train",
                  "producerTask": "feature-eng"
                }
              }
            },
            "parameters": {
              "metrics_names": {
                "runtimeValue": {
                  "constant": [
                    "accuracy_score",
                    "f1_score"
                  ]
                }
              }
            }
          },
          "taskInfo": {
            "name": "Train Model and Save to GCS"
          }
        },
        "extract-table-to-gcs": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-extract-table-to-gcs"
          },
          "dependentTasks": [
            "query-to-table"
          ],
          "inputs": {
            "parameters": {
              "dataset_id": {
                "componentInputParameter": "dataset_id"
              },
              "location": {
                "componentInputParameter": "dataset_location"
              },
              "project_id": {
                "componentInputParameter": "project_id"
              },
              "table_id": {
                "componentInputParameter": "table_id"
              }
            }
          },
          "taskInfo": {
            "name": "Extract Big Query to GCS"
          }
        },
        "feature-eng": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-feature-eng"
          },
          "dependentTasks": [
            "extract-table-to-gcs"
          ],
          "inputs": {
            "artifacts": {
              "data_input": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "dataset",
                  "producerTask": "extract-table-to-gcs"
                }
              }
            },
            "parameters": {
              "col_label": {
                "componentInputParameter": "col_label"
              },
              "col_training": {
                "componentInputParameter": "col_training"
              }
            }
          },
          "taskInfo": {
            "name": "Feature Engineering"
          }
        },
        "query-to-table": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-query-to-table"
          },
          "inputs": {
            "parameters": {
              "dataset_id": {
                "componentInputParameter": "dataset_id"
              },
              "location": {
                "componentInputParameter": "dataset_location"
              },
              "pipelinechannel--dataset_id": {
                "componentInputParameter": "dataset_id"
              },
              "pipelinechannel--project_id": {
                "componentInputParameter": "project_id"
              },
              "pipelinechannel--table_id": {
                "componentInputParameter": "table_id"
              },
              "project_id": {
                "componentInputParameter": "project_id"
              },
              "query": {
                "runtimeValue": {
                  "constant": "SELECT * FROM `{{$.inputs.parameters['pipelinechannel--project_id']}}.{{$.inputs.parameters['pipelinechannel--dataset_id']}}.{{$.inputs.parameters['pipelinechannel--table_id']}}`"
                }
              },
              "query_job_config": {
                "runtimeValue": {
                  "constant": {
                    "write_disposition": "WRITE_TRUNCATE"
                  }
                }
              },
              "table_id": {
                "componentInputParameter": "table_id"
              }
            }
          },
          "taskInfo": {
            "name": "Ingest Data"
          }
        },
        "register-model": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-register-model"
          },
          "dependentTasks": [
            "cross-validation",
            "train-model"
          ],
          "inputs": {
            "artifacts": {
              "model": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "model",
                  "producerTask": "train-model"
                }
              },
              "model_metrics": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "kpi",
                  "producerTask": "cross-validation"
                }
              }
            },
            "parameters": {
              "display_name": {
                "runtimeValue": {
                  "constant": "mf-oxheart-prototype"
                }
              },
              "location": {
                "componentInputParameter": "dataset_location"
              },
              "model_description": {
                "runtimeValue": {
                  "constant": "A prototype model trained for Oxheart"
                }
              },
              "project_id": {
                "componentInputParameter": "project_id"
              }
            }
          },
          "taskInfo": {
            "name": "Register Model in Vertex AI"
          }
        },
        "train-model": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-train-model"
          },
          "dependentTasks": [
            "feature-eng"
          ],
          "inputs": {
            "artifacts": {
              "training_data": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "dataset_train",
                  "producerTask": "feature-eng"
                }
              }
            }
          },
          "taskInfo": {
            "name": "Train Model and Save to GCS"
          }
        }
      }
    },
    "inputDefinitions": {
      "parameters": {
        "col_label": {
          "parameterType": "STRING"
        },
        "col_training": {
          "parameterType": "LIST"
        },
        "dataset_id": {
          "parameterType": "STRING"
        },
        "dataset_location": {
          "parameterType": "STRING"
        },
        "project_id": {
          "parameterType": "STRING"
        },
        "table_id": {
          "parameterType": "STRING"
        }
      }
    },
    "outputDefinitions": {
      "artifacts": {
        "cross-validation-eval_metrics": {
          "artifactType": {
            "schemaTitle": "system.Metrics",
            "schemaVersion": "0.0.1"
          }
        },
        "cross-validation-kpi": {
          "artifactType": {
            "schemaTitle": "system.Metrics",
            "schemaVersion": "0.0.1"
          }
        }
      }
    }
  },
  "schemaVersion": "2.1.0",
  "sdkVersion": "kfp-2.7.0"
}