import json
import os
from typing import NamedTuple
from datetime import datetime
import google.cloud.aiplatform as aiplatform
from kfp import dsl, compiler
from kfp.dsl import (
    component,
    Input,
    Model,
    Output,
    Dataset,
    Artifact,
    OutputPath,
    ClassificationMetrics,
    Metrics,
    InputPath,
)

from arguments import parse_args

# Load secrets from env variables
PROJECT_ID = os.getenv("GCP_PROJECT_ID")

args = parse_args()
GCP_BIGQUERY = "google-cloud-bigquery==3.20.1"
PANDAS = "pandas==2.2.2"
SKLEARN = "scikit-learn==1.4.2"
NUMPY = "numpy==1.26.4"
BASE_IMAGE = "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu.py310"
GOOGLE_CLOUD_STORAGE = "google-cloud-storage"
GOOGLE_CLOUD_AI_PLATFORM = "google-cloud-aiplatform==1.48.0"

TIMESTAMP = datetime.now().strftime("%Y%m%d%H%M%S")
JOBID = f"{args.PIPELINE_NAME}-{TIMESTAMP}"
TEMPLATE_PATH = f"{args.PIPELINE_NAME}.json"


@component(base_image=BASE_IMAGE, packages_to_install=[GCP_BIGQUERY])
def query_to_table(
    query: str,
    project_id: str,
    dataset_id: str,
    table_id: str,
    location: str = "EU",
    query_job_config: dict = None,
) -> None:
    """
    Run the query and create a new BigQuery table
    """

    import google.cloud.bigquery as bq

    # Configure query job
    job_config = bq.QueryJobConfig(
        destination=f"{project_id}.{dataset_id}.{table_id}", **query_job_config
    )

    # Initiate BigQuery client
    bq_client = bq.Client(project=project_id, location=location)

    # Generate query with all job configs
    query_job = bq_client.query(query, job_config=job_config)
    query_job.result()

    print(f"Query job with ID {query_job.job_id} finished.")


@component(base_image=BASE_IMAGE, packages_to_install=[GCP_BIGQUERY])
def extract_table_to_gcs(
    project_id: str,
    dataset_id: str,
    table_id: str,
    dataset: Output[Dataset],
    location: str = "EU",
) -> None:
    """
    Extract a Big Query table into Google Cloud Storage.
    """

    import logging
    import os
    import google.cloud.bigquery as bq

    # Get table generated by previous component
    full_table_id = f"{project_id}.{dataset_id}.{table_id}"
    table = bq.table.Table(table_ref=full_table_id)

    # Initiate BigQuery client to connect with project
    job_config = bq.job.ExtractJobConfig(**{})
    client = bq.client.Client(project=project_id, location=location)

    # Submit extract table job to store on GCS
    extract_job = client.extract_table(table, dataset.uri)


@component(base_image=BASE_IMAGE, packages_to_install=[PANDAS, SKLEARN])
def feature_eng(
    data_input: Input[Dataset],
    dataset_train: OutputPath(),
    col_label: str,
    col_training: list,
) -> None:
    """
    Split data into train and test sets.
    """

    import logging
    import pickle

    import pandas as pd
    from sklearn import model_selection

    df = pd.read_csv(data_input.path)

    # Filling missing max_heart_rate values with median
    df["max_heart_rate"].fillna(df["max_heart_rate"].median(), inplace=True)

    # Some data validation assertions
    # Check for conservative known ranges
    assert df["age"].between(0, 130).all(), "Age is out of the expected range"
    assert (
        df["resting_blood_pressure"].between(0, 400).all()
    ), "Resting blood pressure is out of the expected range"
    assert df["chol"].between(0, 1200).all(), "Cholesterol is out of the expected range"
    assert (
        df["max_heart_rate"].between(0, 600).all()
    ), "Max heart rate is out of the expected range"
    # Check for any missing values in the DataFrame after we've dealt with known missing values
    assert not df.isnull().any().any(), "There are missing values in the DataFrame"

    logging.info(f"[START] CREATE SETS, starts with an initial shape of {df.shape}")

    if len(df) != 0:
        yy = df[col_label]

        xx = df[col_training]

        x_train_results = {"x_train": xx, "y_train": yy}

        with open(dataset_train + f".pkl", "wb") as file:
            pickle.dump(x_train_results, file)

        logging.info(f"[END] FEATURE ENG, training data set was created")

        # return (len(xx))

    else:
        logging.error(f"[END] FEATURE ENG, data set is empty")
        # return (None)


@component(base_image=BASE_IMAGE, packages_to_install=[SKLEARN, PANDAS])
def cross_validation(
    training_data: InputPath(),
    metrics_names: list,
    scores: Output[Metrics],
    eval_metrics: Output[Metrics],
) -> None:
    """
    Train a classification model and save it to Google Cloud Storage.
    """
    import logging
    import os
    import pickle
    import json
    import numpy as np
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import cross_val_score

    logging.basicConfig(level=logging.INFO)

    # Load the training data
    with open(training_data + ".pkl", "rb") as file:
        train_data = pickle.load(file)

    X_train = np.array(train_data["x_train"], copy=True)
    y_train = np.array(train_data["y_train"], copy=True)

    logging.info(f"X_train shape: {X_train.shape}")
    logging.info(f"y_train shape: {y_train.shape}")

    logging.info("Starting training...")
    clf = LogisticRegression(n_jobs=-1, random_state=42)

    metrics_dict = {}
    for each_metric in metrics_names:
        metric_scores = cross_val_score(
            clf, X_train, y_train, cv=5, scoring=each_metric
        )
        metric_val = metric_scores.mean()
        metrics_dict[f"{each_metric}"] = metric_val
        scores.log_metric(f"{each_metric}", metric_val)

        # dumping scores metadata to generate the metrics scores
        with open(scores.path, "w") as f:
            json.dump(scores.metadata, f)
        logging.info(f"{each_metric}: {metric_val:.3f}")

    # dumping metrics_dict to generate the metrics table
    with open(eval_metrics.path, "w") as f:
        json.dump(metrics_dict, f)


@component(base_image=BASE_IMAGE, packages_to_install=[SKLEARN, PANDAS])
def train_model(
    training_data: InputPath(),
    model: Output[Model],
) -> None:
    """
    Train a classification model and save it to Google Cloud Storage.
    """
    import logging
    import os
    import pickle
    import joblib
    import numpy as np
    from sklearn.linear_model import LogisticRegression
    # from google.cloud import storage

    logging.basicConfig(level=logging.INFO)

    # Load the training data
    with open(training_data + ".pkl", "rb") as file:
        train_data = pickle.load(file)

    X_train = np.array(train_data["x_train"], copy=True)
    y_train = np.array(train_data["y_train"], copy=True)

    logging.info(f"X_train shape: {X_train.shape}")
    logging.info(f"y_train shape: {y_train.shape}")

    logging.info("Starting training...")
    clf = LogisticRegression(n_jobs=-1, random_state=42)
    train_model = clf.fit(X_train, y_train)

    # Save the model to a temporary local file
    temp_model_file = "/tmp/model.joblib"
    joblib.dump(train_model, temp_model_file)

    os.makedirs(model.path, exist_ok=True)
    model_file = os.path.join(model.path, "model.joblib")
    # Save the final model
    logging.info(f"Saving model to: {model_file}")
    joblib.dump(train_model, model_file)


@component(
    base_image=BASE_IMAGE, packages_to_install=[PANDAS, GOOGLE_CLOUD_AI_PLATFORM]
)
def register_model(
    model: Input[Model],
    model_metrics: Input[Metrics],
    project_id: str,
    location: str,
    display_name: str,
    model_description: str,
) -> None:
    from google.cloud import aiplatform
    from google.cloud.aiplatform import gapic
    import json

    # Load metrics
    with open(model_metrics.path) as f:
        metrics = json.load(f)

    # Initialize Vertex AI client
    aiplatform.init(project=project_id, location=location)

    # Upload the model to Vertex AI with metrics as labels
    registered_model = aiplatform.Model.upload(
        display_name=display_name,
        artifact_uri=model.path,
        description=model_description,
        serving_container_image_uri="us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-23:latest",
    )

    print(f"Model registered with metrics. ID: {registered_model.resource_name}")


@dsl.pipeline(name=args.PIPELINE_NAME, pipeline_root=args.PIPELINE_ROOT)
def oxheart_prototype_pipeline(
    project_id: str,
    dataset_location: str,
    dataset_id: str,
    table_id: str,
    col_label: str,
    col_training: list,
    metrics: list,
):
    QUERY = f"""SELECT * FROM `{project_id}.{dataset_id}.{table_id}`"""

    ingest = query_to_table(
        query=QUERY,
        table_id=table_id,
        project_id=project_id,
        dataset_id=dataset_id,
        location=dataset_location,
        query_job_config={"write_disposition": "WRITE_TRUNCATE"},
    ).set_display_name("Ingest Data")

    # From big query store in GCS
    ingested_dataset = (
        extract_table_to_gcs(
            project_id=project_id,
            dataset_id=dataset_id,
            table_id=table_id,
            location=dataset_location,
        )
        .after(ingest)
        .set_display_name("Extract Big Query to GCS")
    )

    # Split data
    training_data = feature_eng(
        data_input=ingested_dataset.outputs["dataset"],
        col_label=col_label,
        col_training=col_training,
    ).set_display_name("Feature Engineering")

    # Add to pipeline function
    cv_eval = cross_validation(
        training_data=training_data.outputs["dataset_train"],
        metrics_names=metrics,
    ).set_display_name("Cross Validation")

    training_model = train_model(
        training_data=training_data.outputs["dataset_train"],
    ).set_display_name("Train Model and Save to GCS")

    reg_model = register_model(
        model=training_model.outputs["model"],
        model_metrics=cv_eval.outputs["scores"],
        project_id=project_id,
        location=dataset_location,
        display_name=args.PIPELINE_NAME,
        model_description="A prototype model trained for Oxheart",
    ).set_display_name("Register Model in Vertex AI")


if __name__ == "__main__":
    SERVICE_ACCOUNT_KEY = json.loads(os.getenv("GCP_SA_KEY"))
    SERVICE_ACCOUNT = SERVICE_ACCOUNT_KEY["client_email"]

    PIPELINE_PARAMS = {
        "project_id": PROJECT_ID,
        "dataset_location": args.LOCATION,
        "table_id": args.TABLE_ID,
        "dataset_id": args.DATASET_ID,
        "col_label": args.COL_LABEL,
        "col_training": args.COL_TRAINING,
        "metrics": args.METRICS,
    }

    compiler.Compiler().compile(
        pipeline_func=oxheart_prototype_pipeline, package_path=TEMPLATE_PATH
    )

    aiplatform.init(project=PROJECT_ID, location=args.LOCATION)

    pipeline_ = aiplatform.pipeline_jobs.PipelineJob(
        enable_caching=args.ENABLE_CACHING,
        display_name=args.PIPELINE_NAME,
        template_path=TEMPLATE_PATH,
        job_id=JOBID,
        parameter_values=PIPELINE_PARAMS,
    )

    pipeline_.submit(service_account=SERVICE_ACCOUNT)
